{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import GPT2Tokenizer, GPT2Model, GPT2LMHeadModel\n",
    "import numpy as np\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First let's prepare a tokenized input with GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.tokenization_gpt2:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /home/vili/.pytorch_pretrained_bert/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
      "INFO:pytorch_pretrained_bert.tokenization_gpt2:loading merges file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /home/vili/.pytorch_pretrained_bert/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Encode some inputs\n",
    "text_1 = \"Do not tell him about the company secrets. Or my mum. Because I have an affair with my mum. \"\n",
    "text_2 = \"Do not tell about my mum\"\n",
    "indexed_tokens_1 = tokenizer.encode(text_1)\n",
    "indexed_tokens_2 = tokenizer.encode(text_2)\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor_1 = torch.tensor([indexed_tokens_1])\n",
    "tokens_tensor_2 = torch.tensor([indexed_tokens_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 9])\n",
      "torch.Size([1, 6])\n"
     ]
    }
   ],
   "source": [
    "print(tokens_tensor_1.shape)\n",
    "print(tokens_tensor_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see how to use GPT2Model to get hidden states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.modeling_gpt2:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin from cache at /home/vili/.pytorch_pretrained_bert/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n",
      "INFO:pytorch_pretrained_bert.modeling_gpt2:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /home/vili/.pytorch_pretrained_bert/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.085d5f6a8e7812ea05ff0e6ed0645ab2e75d80387ad55c1ad9806ee70d272f80\n",
      "INFO:pytorch_pretrained_bert.modeling_gpt2:Model config {\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "model.eval()\n",
    "\n",
    "# If you have a GPU, put everything on cuda\n",
    "tokens_tensor_1 = tokens_tensor_1 #.to('cuda')\n",
    "tokens_tensor_2 = tokens_tensor_2 #.to('cuda')\n",
    "#model.to('cuda')\n",
    "\n",
    "# Predict hidden states features for each layer\n",
    "with torch.no_grad():\n",
    "    hidden_states_1, past = model(tokens_tensor_1)\n",
    "    # past can be used to reuse precomputed hidden state in a subsequent predictions\n",
    "    # (see beam-search examples in the run_gpt2.py example).\n",
    "    hidden_states_2, past = model(tokens_tensor_2, past=past)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9, 768])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And how to use GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.modeling_gpt2:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin from cache at /home/vili/.pytorch_pretrained_bert/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n",
      "INFO:pytorch_pretrained_bert.modeling_gpt2:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /home/vili/.pytorch_pretrained_bert/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.085d5f6a8e7812ea05ff0e6ed0645ab2e75d80387ad55c1ad9806ee70d272f80\n",
      "INFO:pytorch_pretrained_bert.modeling_gpt2:Model config {\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.eval()\n",
    "\n",
    "# If you have a GPU, put everything on cuda\n",
    "tokens_tensor_1 = tokens_tensor_1 #.to('cuda')\n",
    "tokens_tensor_2 = tokens_tensor_2 #.to('cuda')\n",
    "#model.to('cuda')\n",
    "\n",
    "# Predict all tokens\n",
    "with torch.no_grad():\n",
    "    predictions_1, past = model(tokens_tensor_1)\n",
    "    # past can be used to reuse precomputed hidden state in a subsequent predictions\n",
    "    # (see beam-search examples in the run_gpt2.py example).\n",
    "    predictions_2, past = model(tokens_tensor_2, past=past)\n",
    "\n",
    "# get the predicted last token\n",
    "predicted_index = torch.argmax(predictions_2[0, -1, :]).item()\n",
    "predicted_token = tokenizer.decode([predicted_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understand the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_preds(predictions, how_many = 10, verbose = False):\n",
    "    for i in range(predictions.shape[1]):\n",
    "        print(f\"****** DIM {i} *********\")\n",
    "        top_10 = np.argpartition(predictions[0,i,:], -how_many)[-how_many:]\n",
    "        if (verbose):\n",
    "            print(\"the most probable was\\n\\t\")\n",
    "        print(tokenizer.decode([torch.argmax(predictions[0, i, :]).item()]), \"\\n\")\n",
    "        if (verbose):\n",
    "            for ind in top_10:\n",
    "                ind = ind.item()\n",
    "                print(ind, \"\\t\", tokenizer.decode([ind]), \"\\t\\twith logprob\\t\", predictions[0,i,ind].item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted was:  13 = . \n",
      "\n",
      "Do not tell him about the company secrets. Or my mum. Because I have an affair with my mum. \n",
      "****** DIM 0 *********\n",
      ". \n",
      "\n",
      "****** DIM 1 *********\n",
      " be \n",
      "\n",
      "****** DIM 2 *********\n",
      " anyone \n",
      "\n",
      "****** DIM 3 *********\n",
      " that \n",
      "\n",
      "****** DIM 4 *********\n",
      " the \n",
      "\n",
      "****** DIM 5 *********\n",
      " fact \n",
      "\n",
      "****** DIM 6 *********\n",
      "'s \n",
      "\n",
      "****** DIM 7 *********\n",
      ". \n",
      "\n",
      "****** DIM 8 *********\n",
      " He \n",
      "\n",
      "****** DIM 9 *********\n",
      " the \n",
      "\n",
      "****** DIM 10 *********\n",
      " personal \n",
      "\n",
      "****** DIM 11 *********\n",
      "'s \n",
      "\n",
      "****** DIM 12 *********\n",
      " Or \n",
      "\n",
      "****** DIM 13 *********\n",
      " I \n",
      "\n",
      "****** DIM 14 *********\n",
      "'m \n",
      "\n",
      "****** DIM 15 *********\n",
      " a \n",
      "\n",
      "****** DIM 16 *********\n",
      " idea \n",
      "\n",
      "****** DIM 17 *********\n",
      " with \n",
      "\n",
      "****** DIM 18 *********\n",
      " a \n",
      "\n",
      "****** DIM 19 *********\n",
      " mum \n",
      "\n",
      "****** DIM 20 *********\n",
      ". \n",
      "\n",
      "****** DIM 21 *********\n",
      " I \n",
      "\n",
      "****** DIM 22 *********\n",
      "Â  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"predicted was: \", predicted_index, \"=\", predicted_token, \"\\n\")\n",
    "\n",
    "print(text_1)\n",
    "print_preds(predictions_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do not tell about my mum\n",
      "****** DIM 0 *********\n",
      "the most probable was\n",
      "\t\n",
      " not \n",
      "\n",
      "1560 \t  tell \t\twith logprob\t -24.417373657226562\n",
      "314 \t  I \t\twith logprob\t -24.355981826782227\n",
      "340 \t  it \t\twith logprob\t -24.340789794921875\n",
      "407 \t  not \t\twith logprob\t -19.683780670166016\n",
      "345 \t  you \t\twith logprob\t -21.48737144470215\n",
      "****** DIM 1 *********\n",
      "the most probable was\n",
      "\t\n",
      " tell \n",
      "\n",
      "1309 \t  let \t\twith logprob\t -143.04208374023438\n",
      "910 \t  say \t\twith logprob\t -142.8894500732422\n",
      "1561 \t  talk \t\twith logprob\t -142.8363494873047\n",
      "1560 \t  tell \t\twith logprob\t -139.27957153320312\n",
      "1265 \t  ask \t\twith logprob\t -142.6316375732422\n",
      "****** DIM 2 *********\n",
      "the most probable was\n",
      "\t\n",
      " him \n",
      "\n",
      "2687 \t  anyone \t\twith logprob\t -82.21723937988281\n",
      "606 \t  them \t\twith logprob\t -81.89225769042969\n",
      "683 \t  him \t\twith logprob\t -78.52623748779297\n",
      "502 \t  me \t\twith logprob\t -80.52213287353516\n",
      "607 \t  her \t\twith logprob\t -80.43750762939453\n",
      "****** DIM 3 *********\n",
      "the most probable was\n",
      "\t\n",
      " the \n",
      "\n",
      "607 \t  her \t\twith logprob\t -78.84773254394531\n",
      "340 \t  it \t\twith logprob\t -78.62315368652344\n",
      "502 \t  me \t\twith logprob\t -78.77643585205078\n",
      "616 \t  my \t\twith logprob\t -77.1417465209961\n",
      "262 \t  the \t\twith logprob\t -76.83594512939453\n",
      "****** DIM 4 *********\n",
      "the most probable was\n",
      "\t\n",
      " mum \n",
      "\n",
      "1641 \t  family \t\twith logprob\t -101.04320526123047\n",
      "25682 \t  mum \t\twith logprob\t -99.66729736328125\n",
      "9955 \t  dad \t\twith logprob\t -100.38604736328125\n",
      "2988 \t  father \t\twith logprob\t -100.94648742675781\n",
      "2802 \t  mother \t\twith logprob\t -100.93402099609375\n",
      "****** DIM 5 *********\n",
      "the most probable was\n",
      "\t\n",
      ". \n",
      "\n",
      "393 \t  or \t\twith logprob\t -29.648128509521484\n",
      "11 \t , \t\twith logprob\t -29.41963005065918\n",
      "526 \t .\" \t\twith logprob\t -29.312681198120117\n",
      "13 \t . \t\twith logprob\t -27.141117095947266\n",
      "338 \t 's \t\twith logprob\t -27.465669631958008\n"
     ]
    }
   ],
   "source": [
    "print(text_2)\n",
    "print_preds(predictions_2, 5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 4])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how take the indeces of the biggest values\n",
    "arr = np.array([1, 3, 2, 4, 5])\n",
    "how_many = 3\n",
    "indeces = np.argpartition(arr, - how_many) # small to big\n",
    "indeces = indeces[-how_many:] # biggest\n",
    "indeces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
